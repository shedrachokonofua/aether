---
- name: Deploy ollama with GPU support
  hosts: gpu-workstation
  gather_facts: true
  roles:
    - common
  tasks:
    - name: Create required directories
      file:
        path: "{{ item }}"
        state: directory
        mode: "0755"
      loop:
        - ./ollama
        - ./ollama/models

    - name: Create ollama container
      containers.podman.podman_container:
        name: ollama
        image: docker.io/ollama/ollama:0.14.3-rc2
        pull: always
        state: quadlet
        restart_policy: on-failure
        recreate: true
        ports:
          - 11434:11434/tcp
        volumes:
          - /home/aether/ollama/models:/root/.ollama:Z
        device:
          - nvidia.com/gpu=all
        env:
          OLLAMA_HOST: "0.0.0.0:11434"
          OLLAMA_KEEP_ALIVE: "5m"
          OLLAMA_NUM_CTX: "131072"
          OLLAMA_FLASH_ATTENTION: "true"
          OLLAMA_DEBUG: "true"
          OLLAMA_NUM_PARALLEL: 5
          OLLAMA_NUM_GPU: 1
          OLLAMA_GPU_LAYERS: -1
        security_opt:
          - label=disable
        quadlet_options:
          - |
            [Install]
            WantedBy=default.target

    - name: Reload systemd daemon for user
      systemd:
        daemon_reload: yes
        scope: user

    - name: Start ollama service
      systemd:
        name: ollama
        state: restarted
        scope: user

    - name: Wait for ollama to start
      wait_for:
        port: 11434
        host: 127.0.0.1
        timeout: 60

    - name: Check ollama version
      uri:
        url: http://localhost:11434/api/version
        method: GET
      register: ollama_version
      retries: 3
      delay: 5

    - name: Display ollama version
      debug:
        msg: "Ollama is running version: {{ ollama_version.json.version }}"

    - name: Pull default models
      command: podman exec ollama ollama pull {{ item }}
      loop:
        - glm-4.7-flash:q8_0
        - mistral-small3.2:24b-instruct-2506-q8_0
        - qwen3:30b
        - gpt-oss:20b
        - gpt-oss:120b
        - qwen3-embedding:4b-q8_0
        - qwen3-embedding:8b
        - gemma3:27b
        - qwen3:8b
        - qwen3-next:80b-a3b-instruct-q4_K_M
        - qwen3-vl:32b-instruct-q8_0
        - nemotron-3-nano:30b-a3b-q8_0
        - qwen3-coder:30b-a3b-fp16
        - devstral-small-2:24b-instruct-2512-fp16
        - hf.co/Trilogix1/Hugston-microsoft-Fara-7B:F16
      register: model_pull
      changed_when: "'pulling' in model_pull.stdout"
      ignore_errors: yes
